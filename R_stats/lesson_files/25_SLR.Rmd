---
title: "Simple Linear Regression"
subtitle: "Sick of stats? Get in line!"
author: "P. Lombardo"
output:
  html_document:
    df_print: paged
---

## Loading packages, etc.
```{r}
library(tidyverse)
library(patchwork)
library(DescTools)
library(broom)
skincancer<-read.csv('data/StateSkinCancer.csv',header = T,
                     stringsAsFactors = T)
```


## Skincancer Revisited

Consider a linear regression for the `skincancer` data set, where we regress skin cancer mortality per capita (`mortality`) onto the latitude of the state (`latitude`).

Let's visualize this with a regression line!
```{r}
# Place code here.
```

Now let's "fit" the model and grab the coefficients:
```{r}
# Place code here.

```


### Information from the model:

Suppose we have more specific information on the latitude in the town where we live: 42.6 (Gloucester, MA).  What does our model predict will be the skin cancer mortality per capita here?

```{r}
# Place code here.
```

**Answer**: answer here.

Suppose we are Gloucester, MA (Latitude 42.6) to Durham, NC (Latitude 35.99).

* Based on our regression model and this difference in latitude, what kind of difference in skin cancer mortality, on average, should we expect between these locations? 
```{r}
# Place code here.
# change in X =...
```
**Answer:**

**Marginal Change Language:** *For each 1 degree increase in latitude....*


***

### Inference in regression:

When we run a regression, what hypothesis tests do we run?  There are three, but one of them is a repeat. Let's focus on these:

1. Slope
2. Intercept (even this one is only *sometimes* important.)

Let's get the output of these hypothesis tests:
```{r}
# Place code here.
```

**Exercise.** Do we have evidence, from this sample, to suggest that the slope term is not equal to zero?
```{r}
#place code here.
```

**Note**: The slope test is special...


### How well does the linear model "fit" the data?  
##### Let's explore $R^2$!

**A useful interpretation of $R^2$**:

# MAKE CHANGE (Some visuals here?)

Comparing $R^2$ with our new interpretation:
```{r}
#Place code here
# Find  1- variance of residuals / variance of mortality

# compare against our R^2 value.

```


**Answer**: answer here.

#### Residual standard error:
On average, how far (as measured in the $y$-direction) is a typical dot from the prediction made by our regression line?

We estimate this error using the residual standard error (RSE):
```{r}

```

How useful is this predictive line?



## Do we have modeling assumptions here?
Yes... do we have time?


## Bonus!
Is there a notion of a confidence interval *for the line* based on the sampling variation we see in the intercept and slope?

Yes! Bare with me code-wise...
```{r}
#create population
test_x<-rnorm(5000, 60, 15)
test_y<-1.3*test_x + rnorm(5000,0,15)

#Gather population model
pop_model<-lm(test_y~test_x)
N<-length(test_x)

#grab a sample model
samp_ind_1<-sample(1:N,30)
samp_data<-data.frame(xs = test_x[samp_ind_1],
                      ys = test_y[samp_ind_1])
sample_model<-lm(ys~xs, data = samp_data)
cfs<-coef(sample_model)

# hlepful function
get_hat_beta<-function(){
    samp_ind<-sample(1:N,30)
    samp_model<-lm(test_y[samp_ind]~test_x[samp_ind])
    return(coef(samp_model))
}

# loop through to get about coefficieents for
# about 250 regressions run from random samples.
samp_betas<-matrix(numeric(3*250),ncol=3)
for(j in 1:250){
  samp_betas[j,]<-c(get_hat_beta(),paste("sample",j,sep=""))
}
# make it a data frame and clean it up.
samp_betas<-as.data.frame(samp_betas)
names(samp_betas)<-c("beta0","beta1","sampleNum")
samp_betas$beta0<-as.numeric(samp_betas$beta0)
samp_betas$beta1<-as.numeric(samp_betas$beta1)

```

Let's start with the population level model (i.e. the truth!):
```{r}
G1<-ggplot()+
  geom_point(aes(x=test_x, y=test_y),
             color='blue',alpha=.15)+theme_bw()+
    geom_abline(intercept = coef(pop_model)[1],
                slope = coef(pop_model)[2],
                color='red',
                linewidth =.8)

G1
```

Now let's add in the 250 simulated regression fits:
```{r}
for (i in 1:250){
    G1<-G1+geom_abline(intercept = samp_betas[i,1],
                       slope = samp_betas[i,2],
                       color='gray1',
                       alpha = .15)
}

G1 + geom_abline(intercept = coef(pop_model)[1],
                slope = coef(pop_model)[2],
                color='red',
                linewidth =.8)
```


```{r}
ggplot()+
        geom_point(aes(x= test_x,
                   y = test_y),
                   color='blue',
                   alpha = .05)+
    geom_point(data = samp_data,
       aes(x = xs, y = ys))+
    geom_smooth(data = samp_data,
       aes(x = xs, y = ys),
       method = "lm",
                formula = y~x,color='red',
       fill = 'gray',alpha = .8)+
    theme_bw()
```




